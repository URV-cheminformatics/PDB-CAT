{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROGRAM WITH COMMENTS AND EXPLANATION\n",
    "### PDB CLASSIFICATION BEFORE DOCKING\n",
    "#### This is a classification program. It can be used to run PDB files and classify them depending on the covalent, non-covalent bonds, and free enzymes. \n",
    "In the following code cell we're going to import the libraries. This is the first step to start working with the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os  # for working with files and directories\n",
    "import re  # for regular expressions\n",
    "import csv  # for reading and writing CSV files\n",
    "import shutil  # for working with files and directories\n",
    "from Bio.Align import PairwiseAligner  # for pairwise sequence alignment\n",
    "from Bio.PDB import *  # for working with PDB files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Next step, it's to determine the directory. \n",
    "\n",
    "It's important to check if the directory is written correctly.\n",
    "Determine the directory where the PDB files are located. Also, determine where the CSV files and folders will be created.\n",
    "You can modify the folder \"/home/user/folder \" to the one where you have saved the PDB files.\n",
    "Path is where you find information output about the protein.\n",
    "Directory is where you should save the downloaded protein structures.\n",
    "Here you can **modify** the folder where the PDB files are located. You can change **path + \"folder_with_structures\" + \"/\"**  to the folder that has the structures of interest. It must be written inside \" \". Check that the folder name is in red!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, my git repository is sort_and_dock, inside this directory there's a folder named as PLpro_structures where all PLpro structures are found\n",
    "path = \"/home/ariadna/pdb/PLpro/\"           #modify folder\n",
    "directory = path + \"PLpro_structures\" + \"/\" #modify folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code creates a list of all files in a specific directory, excluding any other file types or directories using a list comprehension. Get a list of all the PDB files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to compare sequences for same PDB structures. In PDB file we can extract three-letter code sequences, so we need to created a one-letter code dictionary in order to ease the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map amino acid names to single-letter codes\n",
    "aa_dict = {\n",
    "    'ALA': 'A',\n",
    "    'ARG': 'R',\n",
    "    'ASN': 'N',\n",
    "    'ASP': 'D',\n",
    "    'CYS': 'C',\n",
    "    'GLN': 'Q',\n",
    "    'GLU': 'E',\n",
    "    'GLY': 'G',\n",
    "    'HIS': 'H',\n",
    "    'HIE': 'H',\n",
    "    'HID': 'H',\n",
    "    'ILE': 'I',\n",
    "    'LEU': 'L',\n",
    "    'LYS': 'K',\n",
    "    'MET': 'M',\n",
    "    'PHE': 'F',\n",
    "    'PRO': 'P',\n",
    "    'SER': 'S',\n",
    "    'THR': 'T',\n",
    "    'TRP': 'W',\n",
    "    'TYR': 'Y',\n",
    "    'VAL': 'V'\n",
    "}\n",
    "\n",
    "# Create an empty list to store the amino acid sequences as single-letter codes\n",
    "a_letter_code = []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program uses PairWise in Bio library to align sequences, find gaps, find mismatches and to find identity between two structures. The following lines will be used to define how is going to be the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define aligner variables from PairwiseAligner\n",
    "aligner = PairwiseAligner()\n",
    "aligner.mode = 'global'\n",
    "aligner.match_score = 5\n",
    "aligner.mismatch_score = -3\n",
    "aligner.open_gap_score = -7\n",
    "aligner.extend_gap_score = -2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a csv named **sequence_SEQRES.csv** where you can read the PDB ID, chain, all residues - in 1-letter code -, and those pseudopeptides that cannot be translate to 1-letter code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a boolean variable that tracks whether the header row has been written to the CSV file\n",
    "header_written = False\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = path + 'sequence_SEQRES.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.isfile(csv_file_path):\n",
    "    \n",
    "    # If the CSV file doesn't exist, create it and write the header row\n",
    "    with open(csv_file_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['PDB_ID', 'Chain_ID', 'Residues', 'Not_in_dict'])\n",
    "\n",
    "    # Loop over the files and read their contents\n",
    "    for filename in files:\n",
    "        with open(os.path.join(directory, filename), \"r\") as file:\n",
    "            contents = file.read()\n",
    "\n",
    "            # Parse the contents of the file and create residues_dict\n",
    "            residues_dict = {}\n",
    "            residue_line = \"\"\n",
    "            no_dict_matching = []\n",
    "            \n",
    "            for pdb_line in contents.splitlines():\n",
    "                if re.search('^SEQRES .+ . ', pdb_line):\n",
    "                    pdb_line = re.sub(r'\\s+', ' ', pdb_line).strip()\n",
    "                    chain_id = pdb_line.split(\" \")[2] \n",
    "                    residue_line_tmp = pdb_line.split(\" \")[4:]       \n",
    "                    residue_line = ' '.join(residue_line_tmp)\n",
    "\n",
    "                    # Store the residues for this chain in the dictionary\n",
    "                    if chain_id not in residues_dict:\n",
    "                        residues_dict[chain_id] = residue_line\n",
    "                    else:\n",
    "                        residues_dict[chain_id] += ' ' + residue_line\n",
    "                \n",
    "            # Write the data rows to the CSV file if it exists\n",
    "            with open(csv_file_path, 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                \n",
    "                for chain_id, residues in residues_dict.items():\n",
    "                    residues_list = residues.split()\n",
    "                    aa_list = []\n",
    "                    no_dict_matching = []\n",
    "                    \n",
    "                    for residue in residues_list:\n",
    "                        if residue in aa_dict:\n",
    "                            aa_list += aa_dict[residue]\n",
    "                        else:\n",
    "                            no_dict_matching.append([residue, residues_list.index(residue)])         \n",
    "                    \n",
    "                    csv_row = ''.join(aa_list)\n",
    "                    writer.writerow([filename[3:7], chain_id, csv_row, no_dict_matching])\n",
    "            \n",
    "\n",
    "else:\n",
    "    print(f\"{csv_file_path} already exists. Skipping writing to CSV file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Here we need to choose a structure:\n",
    "- With no mutation\n",
    "- In a complex\n",
    "- Good length (not short, not large)\n",
    "\n",
    "Write it in reference_structure string. Be careful, and follow the instructions in the comments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here if we want to select one of our batch folder, download directly from the directory we're gonna \n",
    "# write pdbXXXX.ent  (depend on ID you choose)\n",
    "# write ^SEQRES .+ X (depend on the chain you choose)\n",
    "reference_structure = 'pdb7jiw.ent'\n",
    "chain_sequence = '^SEQRES .+ A '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pdb file of this reference structure, changing the directory\n",
    "with open(os.path.join(directory + reference_structure)) as file:\n",
    "    contents = file.read()\n",
    "\n",
    "    # seqresA is a temporal variable, in order to secure 3-letter to 1-letter change\n",
    "    seqres_one_chain = ''\n",
    "\n",
    "    for pdb_line in contents.splitlines():\n",
    "        if re.search(chain_sequence, pdb_line):\n",
    "            pdb_line = re.sub(r'\\s+', ' ', pdb_line).strip()\n",
    "            \n",
    "            for i in pdb_line.split(' ')[4:]:\n",
    "                seqres_one_chain = seqres_one_chain + str(aa_dict[i])\n",
    "                seq1 = seqres_one_chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna loop over all sequences in the directory to compare with reference sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2_list = []\n",
    "filename_list = []\n",
    "chain_ID_list = []\n",
    "\n",
    "\n",
    "with open(csv_file_path, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # skip the header row if it exists\n",
    "    \n",
    "    for row in csvreader:\n",
    "        # do something with the row\n",
    "        filename_list.append(row[0])\n",
    "        chain_ID_list.append(row[1])\n",
    "        seq2_list.append(row[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, the program run the alignments and define those that are good structures and those that have some mutation and need to be revised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_file_path, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # skip the header row if it exists\n",
    "    \n",
    "    good_structures = []\n",
    "    mutations = []\n",
    "    \n",
    "    for row in csvreader:\n",
    "        seq2 = row[2]\n",
    "        length = len(seq2)\n",
    "\n",
    "        # Perform pairwise alignment\n",
    "        alignments = aligner.align(seq1, seq2)\n",
    "        alignment = alignments[0] # We select the first alignment. It could be more than 1 aligment with same score\n",
    "        counts = alignment.counts()\n",
    "        identity = '{:.2f}'.format(alignment.counts()[1]*100/(len(alignment[0, :])))\n",
    "        \n",
    "        # Extract the count of mismatches\n",
    "        mismatches = counts[2]\n",
    "        gaps = counts[0]\n",
    "\n",
    "        if mismatches == 0:\n",
    "            good_structures.append([row[0], row[1], row[2], length, gaps, identity, row[3]])\n",
    "\n",
    "        if mismatches != 0:\n",
    "            files_mutations = (row[0], row[1], row[2], row[3])\n",
    "\n",
    "        # Where is the mismatch\n",
    "        n = 0\n",
    "        mismatch_location = []\n",
    "        \n",
    "        for i, j in zip(alignment[0, :], alignment[1, :]):\n",
    "            if i != '-':\n",
    "                n=n+1\n",
    "                if i!=j and j!='-':\n",
    "                    mismatch_location.append(i+str(n)+j)\n",
    "        \n",
    "        if len(mismatch_location) != 0:\n",
    "            mutation_path = path + 'mutation_info.csv'\n",
    "            mutations.append([files_mutations[0], files_mutations[1], files_mutations[2], length, gaps, mismatches, mismatch_location, identity])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ariadna/pdb/PLpro/mutation_info.csv already exists. Skipping writing to CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Check if the CSV file exists\n",
    "if not os.path.isfile(mutation_path):\n",
    "    \n",
    "    # Write the header row\n",
    "    with open(mutation_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['PDB_ID', 'Chain_ID', 'Length', 'Gaps', 'Identity', 'Sequence', 'Mismatches', 'Mismatch_location', 'Not_in_dict'])\n",
    "        for mutation in mutations:\n",
    "            writer.writerow(mutation)\n",
    "            \n",
    "else:\n",
    "    print(f\"{mutation_path} already exists. Skipping writing to CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_structures_path = path + 'good_structures_info.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.isfile(good_structures_path):\n",
    "    \n",
    "    # Write the header row\n",
    "    with open(good_structures_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['PDB_ID', 'Chain_ID', 'Sequence', 'Length', 'Gaps', 'Identity', 'Not_in_dict'])\n",
    "\n",
    "    with open(good_structures_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for structure in good_structures:\n",
    "            writer.writerow(structure)\n",
    "            \n",
    "else:\n",
    "    print(f\"{good_structures_path} already exists. Skipping writing to CSV file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You need to *STOP* here in order to check the two csv: \"Good_structures\" and \"Mutation_info\". \n",
    "\n",
    "You can copy information from mutation_info into good_structures for classify in covalent, non-covalent and free enzymes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dir = path + \"structures_for_docking/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_code_list = []\n",
    "with open(path + 'good_structures_info.csv', 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # skip the header row if it exists\n",
    "    for row in csvreader:\n",
    "        PDB_code_list.append(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STRUCTURES_FOR_DOCKING\n",
    "if not os.path.exists(filtered_dir):\n",
    "    os.makedirs(filtered_dir)\n",
    "\n",
    "target = filtered_dir\n",
    "origin = directory  \n",
    "\n",
    "# Fetching the list of all the files\n",
    "files = os.listdir(origin)\n",
    "\n",
    "# Fetching all the files to directory\n",
    "for name in PDB_code_list:\n",
    "    shutil.copy(origin+(\"pdb\"+ name+ \".ent\"), target+(\"pdb\"+ name + \".ent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_files = [f for f in os.listdir(filtered_dir) if os.path.isfile(os.path.join(filtered_dir, f))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set the blacklist.txt directory. It shouldn't be modified.\n",
    "To keep the blacklist up-to-date, specify the directory of the text file with the codes that appear in column 8-10 on the HET line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = \"/home/ariadna/2023/PROTIC/blacklist.txt\"\n",
    "\n",
    "with open(blacklist, 'r') as f:\n",
    "    linea = f.read()\n",
    "linea=linea.replace(' ','')\n",
    "blacklist_ID = linea.strip('\\n').split(',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Here, we can also **modify** the residue or the covalent bond we want to look for, changing the string. \n",
    "\n",
    "The \".\" character means that any character could be in the same position. So, for example, in PDB files, ATOM lines are *residue + chain + number of the residue*. You also need to keep the whitespaces in order to get the match. Also, the \"^\" character means that the line starts with the following. For example, ^*LINK* means that the line starts with LINK.\n",
    "\n",
    "Check that the string is in red!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = \"CYS . 111 \" #modify this residue\n",
    "link = \"^LINK .+ CYS . 111 \" #modify the residue of covalent bond, and if you don't know, just leave \"^LINK .+ \""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can find the core of the code. This it shouldn't be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdb7qcm.ent mutada\n",
      "pdb7qcj.ent mutada\n",
      "pdb7qch.ent mutada\n",
      "pdb7qci.ent mutada\n",
      "pdb7qck.ent mutada\n",
      "pdb7qcg.ent mutada\n",
      "pdb7nt4.ent mutada\n",
      "5 3 16\n"
     ]
    }
   ],
   "source": [
    "# Define the three classification lists that we want to obtain.\n",
    "\n",
    "covalent_list = []\n",
    "free_enzymes_list = []\n",
    "non_covalent_list = []\n",
    "\n",
    "# Define the string of information that we want to add to the CSV.\n",
    "\n",
    "information_covalent=\"\"\n",
    "information_free=\"\"\n",
    "information_non_covalent=\"\"\n",
    "\n",
    "# Open a for loop to iterate through all the PDB files in the mentioned directory\n",
    "# Read the files using .decode() for Ubuntu and its encoding reasons.\n",
    "\n",
    "for pdb_file in filtered_files:\n",
    "    with open(os.path.join(filtered_dir, pdb_file), \"rb\") as f:\n",
    "        pdb_info = f.read().decode()\n",
    "        \n",
    "    # Here we will define the variables that will help to perform the \n",
    "    # classification. 'no_bond' refers to having no bond;  'covalent' refers to having a covalent bond; 'in_black' if it corresponds to an element in the blacklist; 'remove' if it is necessary\n",
    "    # to remove the file from the classification; 'possible_noncovalent' to recognize structures with the potential to be\n",
    "    # non-covalent.\n",
    "\n",
    "    no_bond = 1\n",
    "    covalent = 0\n",
    "    in_black = 1\n",
    "    remove = 1\n",
    "    possible_noncovalent = 1\n",
    "\n",
    "    # Open a for loop to iterate through all the lines of each PDB file and propose the conditions.\n",
    "    for line in pdb_info.splitlines():\n",
    "        if re.search(res_1, line):\n",
    "            remove = 0\n",
    "\n",
    "        if re.search(link, line):\n",
    "            line = re.compile(r'(\\s)\\1{1,}').sub(r'\\1', line)\n",
    "            \n",
    "            # Check that the HET_ID appears at position 5, sometimes it appears at position 6!\n",
    "            if line.upper().split(' ')[6] not in blacklist_ID:\n",
    "                covalent = 1\n",
    "                information_covalent = line\n",
    "                break\n",
    "            \n",
    "        if re.search(\"^HET \", line):\n",
    "            no_bond = 0\n",
    "            line = re.compile(r'(\\s)\\1{1,}').sub(r'\\1', line)\n",
    "            separate_lines = line.split(\" \")\n",
    "            separate_lines = separate_lines[0:5]\n",
    "            information_free= line\n",
    "            \n",
    "            if not separate_lines[1] in blacklist_ID:\n",
    "                in_black = 0\n",
    "                information_non_covalent = line\n",
    "\n",
    "\n",
    "   # End of the for loop that iterates through the lines of the file.                \n",
    "    if remove == 1:\n",
    "        print(pdb_file, \"mutada\")\n",
    "    elif covalent == 1:\n",
    "        covalent_list.append([pdb_file[3:7], information_covalent])\n",
    "    elif no_bond == 0 and in_black == 0:\n",
    "        non_covalent_list.append([pdb_file[3:7], information_non_covalent])\n",
    "    else:\n",
    "        free_enzymes_list.append([pdb_file[3:7], information_free])\n",
    "\n",
    "print(len(covalent_list), len(free_enzymes_list), len(non_covalent_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code writes three csv files, one for each group. You can edit the csv name, replacing 'DataCovalent.csv' for another one. **Keep writing .csv** !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_for_each_group(name_of_csv, type_list):\n",
    "    fields = ['PDB ID', 'LINE']\n",
    "    rows = type_list[:]\n",
    "    with open(path + name_of_csv, 'w') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(fields)\n",
    "        csv_writer.writerows(rows)\n",
    "        \n",
    "write_csv_for_each_group('DataCovalent.csv', covalent_list)\n",
    "write_csv_for_each_group('DataNonCovalent.csv', non_covalent_list)\n",
    "write_csv_for_each_group('DataFree.csv', free_enzymes_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A NEW FOLDER TO SAVE THE FILES OF EACH GROUP\n",
    "\n",
    "1. Free_Folder\n",
    "2. NonCovalent_Folder\n",
    "3. Covalent_Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FREE PROTEINS\n",
    "free_list_ID=[item[0] for item in free_enzymes_list]\n",
    "\n",
    "newpath = path + 'Free_Folder/'\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "target = newpath\n",
    "origin = filtered_dir\n",
    "\n",
    "# Fetching the list of all the files\n",
    "files = os.listdir(origin)\n",
    "\n",
    "# Fetching all the files to directory\n",
    "for name in free_list_ID:\n",
    "    shutil.copy(origin+(\"pdb\"+ name+ \".ent\"), target+(\"pdb\"+ name + \".ent\"))\n",
    "    \n",
    "    \n",
    "###NON-COVALENT PROTEINS\n",
    "\n",
    "non_covalent_list_ID=[item[0] for item in non_covalent_list]\n",
    "\n",
    "newpath = path + 'NonCovalent_Folder/'\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "target = newpath\n",
    "# Fetching the list of all the files\n",
    "files = os.listdir(origin)\n",
    "\n",
    "# Fetching all the files to directory\n",
    "for pdb_file in non_covalent_list_ID:\n",
    "    shutil.copy(origin+(\"pdb\"+ pdb_file + \".ent\"), target+(\"pdb\"+ pdb_file + \".ent\"))\n",
    "\n",
    "\n",
    "###COVALENT PROTEINS\n",
    "\n",
    "covalent_list_ID=[item[0] for item in covalent_list]\n",
    "\n",
    "newpath = path + 'Covalent_Folder/'\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "target = newpath\n",
    "\n",
    "# Fetching the list of all the files\n",
    "files = os.listdir(origin)\n",
    "\n",
    "# Fetching all the files to directory\n",
    "for pdb_file in covalent_list_ID:\n",
    "    shutil.copy(origin+(\"pdb\"+ pdb_file + \".ent\"), target+(\"pdb\"+ pdb_file + \".ent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8dbfddbd1fb5cd3192e947e7a5eb6a921df3627cbd91ae771db882486d17a55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
